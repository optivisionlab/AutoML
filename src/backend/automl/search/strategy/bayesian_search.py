from typing import Any, Dict, Tuple
import numpy as np
import pandas as pd
from datetime import datetime
import os
from sklearn.base import BaseEstimator
from sklearn.model_selection import cross_validate
from skopt import gp_minimize
from skopt.space import Categorical
from skopt.utils import use_named_args
from collections import Counter

from automl.search.strategy.base import SearchStrategy

class BayesianSearchStrategy(SearchStrategy):
    """
        Thá»±c thi tÃ¬m kiáº¿m siÃªu tham sá»‘ báº±ng Tá»‘i Æ°u hÃ³a Bayesian (Bayesian Optimization)
        sá»­ dá»¥ng thÆ° viá»‡n scikit-optimize.
    """

    def _detect_class_imbalance(self, y: np.ndarray) -> bool:
        """
        PhÃ¡t hiá»‡n xem táº­p dá»¯ liá»‡u cÃ³ máº¥t cÃ¢n báº±ng lá»›p hay khÃ´ng.
        
        Args:
            y: Máº£ng nhÃ£n
            
        Returns:
            bool: True náº¿u máº¥t cÃ¢n báº±ng, False náº¿u cÃ¢n báº±ng
        """
        # Äáº¿m sá»‘ lÆ°á»£ng cá»§a má»—i lá»›p
        class_counts = Counter(y)
        total_samples = len(y)
        
        # TÃ­nh tá»· lá»‡ cá»§a má»—i lá»›p
        class_ratios = {cls: count/total_samples for cls, count in class_counts.items()}
        
        # TÃ¬m tá»· lá»‡ nhá» nháº¥t vÃ  lá»›n nháº¥t
        min_ratio = min(class_ratios.values())
        max_ratio = max(class_ratios.values())
        
        # Kiá»ƒm tra xem sá»± khÃ¡c biá»‡t cÃ³ Ä‘Ã¡ng ká»ƒ khÃ´ng
        threshold = self.config.get('imbalance_threshold', 0.3)
        
        # Náº¿u chÃªnh lá»‡ch giá»¯a lá»›p nhá» nháº¥t vÃ  lá»›n nháº¥t vÆ°á»£t ngÆ°á»¡ng, dá»¯ liá»‡u máº¥t cÃ¢n báº±ng
        return (max_ratio - min_ratio) > threshold
    
    def _get_averaging_method(self, y: np.ndarray) -> str:
        """
        XÃ¡c Ä‘á»‹nh phÆ°Æ¡ng phÃ¡p tÃ­nh trung bÃ¬nh nÃ o sáº½ sá»­ dá»¥ng dá»±a trÃªn cáº¥u hÃ¬nh vÃ  dá»¯ liá»‡u.
        
        Args:
            y: Máº£ng nhÃ£n
            
        Returns:
            str: 'macro' hoáº·c 'weighted'
        """
        averaging = self.config.get('averaging', 'auto')
        
        if averaging == 'auto':
            # Tá»± Ä‘á»™ng phÃ¡t hiá»‡n dá»±a trÃªn cÃ¢n báº±ng lá»›p
            if self._detect_class_imbalance(y):
                print("PhÃ¡t hiá»‡n máº¥t cÃ¢n báº±ng lá»›p. Sá»­ dá»¥ng trung bÃ¬nh cÃ³ trá»ng sá»‘ (weighted).")
                return 'weighted'
            else:
                print("PhÃ¡t hiá»‡n cÃ¡c lá»›p cÃ¢n báº±ng. Sá»­ dá»¥ng trung bÃ¬nh macro.")
                return 'macro'
        elif averaging in ['macro', 'weighted']:
            print(f"Sá»­ dá»¥ng trung bÃ¬nh {averaging} theo cáº¥u hÃ¬nh.")
            return averaging
        else:
            print(f"PhÆ°Æ¡ng phÃ¡p tÃ­nh trung bÃ¬nh '{averaging}' khÃ´ng há»£p lá»‡. Máº·c Ä‘á»‹nh dÃ¹ng macro.")
            return 'macro'

    @staticmethod
    def get_default_config() -> Dict[str, Any]:
        """Ghi Ä‘Ã¨ cáº¥u hÃ¬nh máº·c Ä‘á»‹nh Ä‘á»ƒ thÃªm cÃ¡c tham sá»‘ cho tá»‘i Æ°u hÃ³a Bayesian"""
        base_config = SearchStrategy.get_default_config()
        bayesian_config = {
            'n_calls': 25,  # Reduced for faster runtime (was 50)
            'n_initial_points': 5,  # Reduced initial random exploration (default is 10)
            'acq_func': 'EI',  # Expected Improvement - faster than default 'gp_hedge'
            'acq_optimizer': 'sampling',  # Faster than 'lbfgs' for acquisition
            'scoring': 'accuracy',  # HÃ m Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh
            'metrics': ['accuracy', 'precision', 'recall', 'f1'],
            'log_dir': 'logs',  # ThÆ° má»¥c lÆ°u log
            'save_log': False,  # Disabled by default for speed
            'averaging': 'macro',  # Single averaging method is faster than 'both'
            'optimize_for': 'auto',  # 'auto', 'macro', 'weighted' 
            'imbalance_threshold': 0.3,  # Threshold for detecting class imbalance (auto mode)
            'early_stopping_enabled': True,  # Enable early stopping
            'early_stopping_patience': 5,  # Stop if no improvement for these many iterations
            'convergence_threshold': 0.001,  # Stop if improvement < threshold
        }
        base_config.update(bayesian_config)
        return base_config

    def search(self, model: BaseEstimator, param_grid: Dict[str, Any],
               X: np.ndarray, y: np.ndarray, **kwargs) -> Tuple[Dict[str, Any], float, Dict[str, float], Dict[str, Any]]:
        """
        Thá»±c thi thuáº­t toÃ¡n tÃ¬m kiáº¿m.

        Args:
            model (BaseEstimator): MÃ´ hÃ¬nh scikit-learn.
            param_grid (Dict[str, Any]): Má»™t dictionary trong Ä‘Ã³ key lÃ  tÃªn tham sá»‘ vÃ  
                                         value lÃ  má»™t dimension cá»§a skopt (Real, Integer, Categorical).
            X (np.ndarray): Dá»¯ liá»‡u features.
            y (np.ndarray): Dá»¯ liá»‡u target.
            **kwargs: CÃ¡c tham sá»‘ bá»• sung cho gp_minimize.

        Returns:
            Tuple[Dict, float, Dict, Dict]: (best_params, best_score, best_all_scores, cv_results_)
                - best_params: Dictionary of best parameters
                - best_score: Best score achieved
                - best_all_scores: Dictionary with all metric scores for best parameters  
                - cv_results_: Dictionary with detailed cross-validation results
        """
        self.set_config(**kwargs)

        # Táº¡o thÆ° má»¥c log náº¿u cáº§n
        if self.config['save_log']:
            log_dir = self.config['log_dir']
            os.makedirs(log_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = model.__class__.__name__
            log_file = os.path.join(log_dir, f'bayesian_search_{model_name}_{timestamp}.csv')

        # Chuyá»ƒn Ä‘á»•i param_grid thÃ nh danh sÃ¡ch cÃ¡c dimension
        search_space = []
        param_names = []

        for param_name, param_value in param_grid.items():
            # Náº¿u param_value lÃ  dimension object (Real, Integer, Categorical)
            if hasattr(param_value, 'name'):
                if param_value.name is None:
                    param_value.name = param_name
                search_space.append(param_value)
                param_names.append(param_name)
            # Náº¿u param_value lÃ  list hoáº·c tuple, chuyá»ƒn thÃ nh Categorical
            elif isinstance(param_value, (list, tuple)):
                dim = Categorical(param_value, name=param_name)
                search_space.append(dim)
                param_names.append(param_name)
            else:
                raise ValueError(f"Invalid parameter type for {param_name}: {type(param_value)}")

        # Danh sÃ¡ch Ä‘á»ƒ lÆ°u lá»‹ch sá»­ tÃ¬m kiáº¿m
        search_history = []
        
        # Initialize cv_results_ dictionary similar to grid_search
        cv_results_ = {
            'params': [],
            'mean_test_score': [],
            'std_test_score': [],
            'rank_test_score': []
        }
        
        # Add metric-specific fields
        metrics = self.config.get('metrics', ['accuracy', 'precision', 'recall', 'f1'])
        for metric in metrics:
            cv_results_[f'mean_test_{metric}'] = []
            cv_results_[f'std_test_{metric}'] = []
            cv_results_[f'rank_test_{metric}'] = []
        
        # Track best metrics for all scores
        best_all_scores = None
        
        # Determine which metrics to compute
        averaging = self.config.get('averaging', 'both')
        optimize_for = self.config.get('optimize_for', 'auto')
        
        # If optimize_for is 'auto', detect based on class balance
        if optimize_for == 'auto':
            optimize_for = 'weighted' if self._detect_class_imbalance(y) else 'macro'
            print(f"Auto-detected optimization target: {optimize_for}")

        # Äá»‹nh nghÄ©a hÃ m má»¥c tiÃªu
        @use_named_args(search_space)
        def objective(**params):
            model.set_params(**params)

            scoring = self.config['scoring']
            metrics = self.config.get('metrics', ['accuracy', 'precision', 'recall', 'f1'])

            # Build scoring metrics dict based on averaging setting
            scoring_metrics = {}
            
            if averaging == 'both':
                # Compute both macro and weighted metrics
                scoring_metrics['accuracy'] = 'accuracy'
                for metric in ['precision', 'recall', 'f1']:
                    if metric in metrics:
                        scoring_metrics[f'{metric}_macro'] = f'{metric}_macro'
                        scoring_metrics[f'{metric}_weighted'] = f'{metric}_weighted'
                
                # Add the primary scoring metric if not already included
                if scoring != 'accuracy' and scoring in metrics:
                    if f'{scoring}_macro' not in scoring_metrics:
                        scoring_metrics[f'{scoring}_macro'] = f'{scoring}_macro'
                    if f'{scoring}_weighted' not in scoring_metrics:
                        scoring_metrics[f'{scoring}_weighted'] = f'{scoring}_weighted'
                        
            else:
                # Use only specified averaging method
                avg_method = averaging if averaging in ['macro', 'weighted'] else 'macro'
                scoring_metrics['accuracy'] = 'accuracy'
                for metric in metrics:
                    if metric == 'accuracy':
                        continue
                    scoring_metrics[metric] = f'{metric}_{avg_method}'

            cv_results = cross_validate(
                estimator=model,
                X=X,
                y=y,
                cv=self.config['cv'],
                n_jobs=self.config['n_jobs'],
                scoring=scoring_metrics,
                error_score=self.config['error_score'],
                return_train_score=False
            )

            # Store all metrics - will be converted later
            objective.last_metrics = {}
            
            if averaging == 'both':
                # Store both macro and weighted metrics - convert numpy types
                objective.last_metrics['accuracy'] = float(np.mean(cv_results['test_accuracy']))
                
                for metric in ['precision', 'recall', 'f1']:
                    if f'{metric}_macro' in scoring_metrics:
                        objective.last_metrics[f'{metric}_macro'] = float(np.mean(cv_results[f'test_{metric}_macro']))
                    if f'{metric}_weighted' in scoring_metrics:
                        objective.last_metrics[f'{metric}_weighted'] = float(np.mean(cv_results[f'test_{metric}_weighted']))
                
                # Choose which score to optimize based on configuration
                if scoring == 'accuracy':
                    score = objective.last_metrics['accuracy']
                else:
                    score = objective.last_metrics.get(f'{scoring}_{optimize_for}', 
                                                      objective.last_metrics.get('accuracy', 0.0))
            else:
                # Single averaging method - convert numpy types
                for key in scoring_metrics:
                    objective.last_metrics[key] = float(np.mean(cv_results[f'test_{key}']))
                score = objective.last_metrics.get(scoring, objective.last_metrics.get('accuracy', 0.0))

            # gp_minimize luÃ´n tá»‘i thiá»ƒu hÃ³a, vÃ¬ váº­y tráº£ vá» -score
            return -score

        # Track for early stopping
        best_score_history = []
        early_stopping_patience = self.config.get('early_stopping_patience', 5)
        early_stopping_enabled = self.config.get('early_stopping_enabled', True)
        convergence_threshold = self.config.get('convergence_threshold', 0.001)
        
        # HÃ m callback Ä‘á»ƒ lÆ°u lá»‹ch sá»­
        def on_step(res):
            """Callback Ä‘Æ°á»£c gá»i sau má»—i iteration Ä‘á»ƒ lÆ°u káº¿t quáº£"""
            nonlocal best_all_scores, best_score_history
            
            iteration = len(res.x_iters)
            # Convert numpy types to native Python types using base class method
            raw_params = {}
            for i, val in enumerate(res.x_iters[-1]):
                raw_params[param_names[i]] = val
            # Use the base class converter for comprehensive conversion
            current_params = SearchStrategy.convert_numpy_types(raw_params)
            
            current_score = float(-res.func_vals[-1]) if hasattr(res.func_vals[-1], 'item') else -res.func_vals[-1]
            best_score_so_far = float(-res.fun) if hasattr(res.fun, 'item') else -res.fun

            # Láº¥y tÃªn model
            model_name = model.__class__.__name__

            # Láº¥y metrics tá»« iteration hiá»‡n táº¡i vÃ  convert numpy types
            raw_metrics = getattr(objective, 'last_metrics', {})
            metrics = SearchStrategy.convert_numpy_types(raw_metrics)
            
            # Táº¡o báº£n ghi cho lá»‹ch sá»­
            record = {
                'model': model_name,
                'run_type': 'bayesian_search',
                'best_params': str(current_params),
                'accuracy': metrics.get('accuracy', 0.0),
            }
            
            # ThÃªm metrics dá»±a trÃªn cáº¥u hÃ¬nh averaging
            if averaging == 'both':
                # ThÃªm cáº£ metric macro vÃ  weighted
                for metric_type in ['precision', 'recall', 'f1']:
                    record[f'{metric_type}_macro'] = metrics.get(f'{metric_type}_macro', 0.0)
                    record[f'{metric_type}_weighted'] = metrics.get(f'{metric_type}_weighted', 0.0)
                
                # In thÃ´ng tin ra console vá»›i cáº£ hai loáº¡i
                print(f"Láº§n láº·p {iteration}/{self.config['n_calls']}: ")
                print(f"  Äá»™ chÃ­nh xÃ¡c={metrics.get('accuracy', 0.0):.4f}")
                print(f"  Macro   - P={metrics.get('precision_macro', 0.0):.4f}, "
                      f"R={metrics.get('recall_macro', 0.0):.4f}, "
                      f"F1={metrics.get('f1_macro', 0.0):.4f}")
                print(f"  Weighted- P={metrics.get('precision_weighted', 0.0):.4f}, "
                      f"R={metrics.get('recall_weighted', 0.0):.4f}, "
                      f"F1={metrics.get('f1_weighted', 0.0):.4f}")
                print(f"  Äang tá»‘i Æ°u cho: {optimize_for}")
                
            else:
                # PhÆ°Æ¡ng phÃ¡p averaging Ä‘Æ¡n
                avg_suffix = '' if averaging not in ['macro', 'weighted'] else f'_{averaging}'
                for metric_type in ['precision', 'recall', 'f1']:
                    key = f'{metric_type}{avg_suffix}' if avg_suffix else metric_type
                    record[metric_type] = metrics.get(key, 0.0)
                
                # In thÃ´ng tin ra console
                print(f"Láº§n láº·p {iteration}/{self.config['n_calls']}: "
                      f"Äá»™ chÃ­nh xÃ¡c={metrics.get('accuracy', 0.0):.4f}, "
                      f"Precision={record.get('precision', 0.0):.4f}, "
                      f"Recall={record.get('recall', 0.0):.4f}, "
                      f"F1={record.get('f1', 0.0):.4f}")
            
            search_history.append(record)
            
            # Populate cv_results_
            cv_results_['params'].append(current_params)
            cv_results_['mean_test_score'].append(current_score)
            cv_results_['std_test_score'].append(0.0)  # Bayesian opt doesn't compute std per iteration
            
            # Add metrics to cv_results_
            cv_results_['mean_test_accuracy'].append(metrics.get('accuracy', 0.0))
            cv_results_['std_test_accuracy'].append(0.0)
            
            if averaging == 'both':
                # Add both macro and weighted metrics
                for metric_type in ['precision', 'recall', 'f1']:
                    # For compatibility, use the optimized version
                    if optimize_for == 'macro':
                        cv_results_[f'mean_test_{metric_type}'].append(metrics.get(f'{metric_type}_macro', 0.0))
                    else:
                        cv_results_[f'mean_test_{metric_type}'].append(metrics.get(f'{metric_type}_weighted', 0.0))
                    cv_results_[f'std_test_{metric_type}'].append(0.0)
            else:
                # Single averaging method
                avg_suffix = '' if averaging not in ['macro', 'weighted'] else f'_{averaging}'
                for metric_type in ['precision', 'recall', 'f1']:
                    key = f'{metric_type}{avg_suffix}' if avg_suffix else metric_type
                    cv_results_[f'mean_test_{metric_type}'].append(metrics.get(key, 0.0))
                    cv_results_[f'std_test_{metric_type}'].append(0.0)
            
            # Update best_all_scores if this is the best so far
            if current_score >= best_score_so_far:
                best_all_scores = SearchStrategy.convert_numpy_types(metrics.copy())
            
            # Track score history for early stopping
            best_score_history.append(best_score_so_far)
            
            # Check early stopping conditions
            if early_stopping_enabled and iteration >= early_stopping_patience:
                # Check if no improvement for patience iterations
                recent_scores = best_score_history[-early_stopping_patience:]
                if len(set(recent_scores)) == 1:  # No improvement
                    print(f"\nðŸ›‘ Early stopping at iteration {iteration} (no improvement for {early_stopping_patience} iterations)")
                    return True  # This will stop gp_minimize
                
                # Check convergence threshold
                if len(best_score_history) > 1:
                    recent_improvement = best_score_history[-1] - best_score_history[-2]
                    if abs(recent_improvement) < convergence_threshold:
                        print(f"\nâœ“ Convergence detected at iteration {iteration} (improvement < {convergence_threshold:.4f})")
                        return True

            # LÆ°u log sau má»—i iteration náº¿u Ä‘Æ°á»£c yÃªu cáº§u
            if self.config['save_log']:
                df = pd.DataFrame(search_history)
                df.to_csv(log_file, index=False)

        # Lá»c kwargs Ä‘á»ƒ chá»‰ giá»¯ cÃ¡c tham sá»‘ há»£p lá»‡ cho gp_minimize
        # Loáº¡i bá» cÃ¡c tham sá»‘ cá»§a SearchStrategy nhÆ° 'scoring', 'cv', 'n_jobs', 'error_score'
        valid_gp_minimize_params = ['n_initial_points', 'acq_func', 'acq_optimizer',
                                    'x0', 'y0', 'noise', 'n_points', 'n_restarts_optimizer',
                                    'xi', 'kappa', 'verbose', 'callback', 'n_jobs']
        gp_kwargs = {k: v for k, v in kwargs.items() if k in valid_gp_minimize_params}
        
        # Add optimized parameters from config if not already specified
        if 'n_initial_points' not in gp_kwargs:
            gp_kwargs['n_initial_points'] = self.config.get('n_initial_points', 5)
        if 'acq_func' not in gp_kwargs:
            gp_kwargs['acq_func'] = self.config.get('acq_func', 'EI')
        if 'acq_optimizer' not in gp_kwargs:
            gp_kwargs['acq_optimizer'] = self.config.get('acq_optimizer', 'sampling')

        # ThÃªm callback vÃ o gp_kwargs
        if 'callback' in gp_kwargs:
            if isinstance(gp_kwargs['callback'], list):
                gp_kwargs['callback'].append(on_step)
            else:
                gp_kwargs['callback'] = [gp_kwargs['callback'], on_step]
        else:
            gp_kwargs['callback'] = [on_step]

        # Thá»±c thi tá»‘i Æ°u hÃ³a Bayesian with parallel support
        # Ensure n_jobs is set for parallel acquisition function optimization
        if 'n_jobs' not in gp_kwargs:
            import multiprocessing
            gp_kwargs['n_jobs'] = self.config.get('n_jobs', multiprocessing.cpu_count())
        
        result = gp_minimize(
            func=objective,
            dimensions=search_space,
            n_calls=self.config['n_calls'],
            random_state=self.config['random_state'],
            **gp_kwargs
        )

        # Láº¥y tham sá»‘ tá»‘t nháº¥t vÃ  Ä‘iá»ƒm sá»‘ tá»‘t nháº¥t
        # Convert numpy types to native Python types using base class method
        raw_best_params = {}
        for i, val in enumerate(result.x):
            raw_best_params[param_names[i]] = val
        best_params = SearchStrategy.convert_numpy_types(raw_best_params)
        
        best_score = float(-result.fun) if hasattr(result.fun, 'item') else -result.fun  # Äá»•i dáº¥u Ä‘á»ƒ láº¥y giÃ¡ trá»‹ dÆ°Æ¡ng
        
        # Compute rankings for cv_results_
        for metric in metrics:
            test_scores = cv_results_[f'mean_test_{metric}']
            if test_scores:
                ranks = np.argsort(np.argsort(-np.array(test_scores))) + 1
                cv_results_[f'rank_test_{metric}'] = ranks.tolist()
        
        # Overall ranking based on the optimization metric
        test_scores = cv_results_['mean_test_score']
        if test_scores:
            ranks = np.argsort(np.argsort(-np.array(test_scores))) + 1
            cv_results_['rank_test_score'] = ranks.tolist()
        
        # If best_all_scores was not set (shouldn't happen), create it from the final run
        if best_all_scores is None:
            raw_metrics = getattr(objective, 'last_metrics', {})
            best_all_scores = SearchStrategy.convert_numpy_types(raw_metrics)

        # In thÃ´ng bÃ¡o vá» vá»‹ trÃ­ file log
        if self.config['save_log']:
            print(f"\nÄÃ£ lÆ°u log tÃ¬m kiáº¿m vÃ o: {log_file}")

        # Convert all numpy types in cv_results_ to native Python types
        cv_results_ = SearchStrategy.convert_numpy_types(cv_results_)

        return best_params, best_score, best_all_scores, cv_results_
